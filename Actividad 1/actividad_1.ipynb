{"cells": [{"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "15bfcfc880f2897e6836933d55c7c042", "grade": false, "grade_id": "cell-570cf80ae1b2c48e", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "# Actividad 1: HDFS, Spark SQL y MLlib"}, {"cell_type": "markdown", "metadata": {}, "source": "## Recuerda borrar siempre las l\u00edneas que dicen `raise NotImplementedError`"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "7a238d899b5a6e8c414330ade880233f", "grade": false, "grade_id": "cell-f4c598b6fd61ee12", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "Lee con detenimiento cada ejercicio. Las variables utilizadas para almacenar las soluciones, al igual que las nuevas columnas creadas, deben llamarse **exactamente** como indica el ejercicio, o de lo contrario los tests fallar\u00e1n y el ejercicio no puntuar\u00e1. Debe reemplazarse el valor `None` al que est\u00e1n inicializadas por el c\u00f3digo necesario para resolver el ejercicio."}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "10cdca7b33ec4328e542f94942371393", "grade": false, "grade_id": "cell-42368b0202b6ce77", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "## Leemos el fichero flights.csv que hemos subido a HDFS"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "2b86205b9dc8b69adf7aa9ba76518864", "grade": false, "grade_id": "cell-3202a483f423590a", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "Indicamos que contiene encabezados (nombres de columnas) y que intente inferir el esquema, aunque despu\u00e9s comprobaremos si lo\nha inferido correctamente o no. La ruta del archivo en HDFS deber\u00eda ser /<nombre_alumno>/flights.csv"}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://cluster-jde-m.europe-west1-c.c.neon-mesh-333522.internal:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v2.4.8</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7f1948269160>"}, "execution_count": 1, "metadata": {}, "output_type": "execute_result"}], "source": "spark"}, {"cell_type": "code", "execution_count": 2, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "5310bc8c3244ee98391957d26dc91d08", "grade": false, "grade_id": "lectura-fichero", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [{"ename": "AnalysisException", "evalue": "'Path does not exist: hdfs://cluster-jde-m/juan_escobar_escobar/flights_act1.csv;'", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)", "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o63.csv.\n: org.apache.spark.sql.AnalysisException: Path does not exist: hdfs://cluster-jde-m/juan_escobar_escobar/flights_act1.csv;\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:576)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:559)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:559)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:373)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:242)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:230)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:641)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n", "\nDuring handling of the above exception, another exception occurred:\n", "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)", "\u001b[0;32m<ipython-input-2-3c191c03aa50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inferSchema\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruta_hdfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue)\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mAnalysisException\u001b[0m: 'Path does not exist: hdfs://cluster-jde-m/juan_escobar_escobar/flights_act1.csv;'"]}], "source": "ruta_hdfs = \"/juan_escobar_escobar/flights_act1.csv\"\n#ruta_gcs = \"gs://jde_procesado_datos_masivos_act_1/data/flights_act1.csv\"\n\nflightsDF = None\n\n# Descomentar estas l\u00edneas\nflightsDF = spark.read\\\n            .option(\"header\", \"true\")\\\n            .option(\"inferSchema\", \"true\")\\\n            .csv(ruta_hdfs)"}, {"cell_type": "markdown", "metadata": {}, "source": "Imprimimos el esquema para comprobar qu\u00e9 tipo de dato ha inferido en cada columna"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "flightsDF.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "Mostramos el n\u00famero de filas que tiene el DataFrame para hacernos una idea de su tama\u00f1o:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "flightsDF.count()\n#162.049"}, {"cell_type": "markdown", "metadata": {}, "source": "Vemos que tenemos 162049 filas. Si imprimimos por pantalla las 5 primeras filas, veremos qu\u00e9 tipos parecen tener y en qu\u00e9 columnas no coincide el tipo que podr\u00edamos esperar con el tipo que ha inferido Spark."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "flightsDF.show(5)\n\n# |-- year: integer (nullable = true)      OK\n# |-- month: integer (nullable = true)     OK\n# |-- day: integer (nullable = true)       OK\n# |-- dep_time: string (nullable = true)   INTEGER?\n# |-- dep_delay: string (nullable = true)  INTEGER?\n# |-- arr_time: string (nullable = true)   INTEGER?\n# |-- arr_delay: string (nullable = true)  INTEGER?\n# |-- carrier: string (nullable = true)    OK\n# |-- tailnum: string (nullable = true)    OK\n# |-- flight: integer (nullable = true)    OK\n# |-- origin: string (nullable = true)     OK\n# |-- dest: string (nullable = true)       OK\n# |-- air_time: string (nullable = true)   INTEGER?\n# |-- distance: integer (nullable = true)  OK\n# |-- hour: string (nullable = true)       INTEGER? \n# |-- minute: string (nullable = true)     INTEGER?"}, {"cell_type": "markdown", "metadata": {}, "source": "La causa del problema es que en muchas columnas existe un valor faltante llamado \"NA\". Spark no reconoce ese valor como\n*no disponible* ni nada similar, sino que lo considera como un string de texto normal, y por tanto, asigna a toda la columna\nel tipo de dato string (cadena de caracteres). Concretamente, las siguientes columnas deber\u00edan ser de tipo entero pero Spark\nlas muestra como string:\n<ul>\n <li>dep_time: string (nullable = true)\n <li>dep_delay: string (nullable = true)\n <li>arr_time: string (nullable = true)\n <li>arr_delay: string (nullable = true)\n <li>air_time: string (nullable = true)\n <li>hour: string (nullable = true)\n <li>minute: string (nullable = true)    \n</ul>\n"}, {"cell_type": "markdown", "metadata": {}, "source": "Vamos a averiguar cu\u00e1ntas filas tienen el valor \"NA\" (como string) en la columna dep_time:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "from pyspark.sql import functions as F\ncuantos_NA = flightsDF\\\n                .where(F.col(\"dep_time\") == \"NA\")\\\n                .count()\ncuantos_NA"}, {"cell_type": "markdown", "metadata": {}, "source": "Por tanto, hay 857 filas que no tienen un dato v\u00e1lido en esa columna. Hay distintas maneras de trabajar con los valores faltantes, como por ejemplo imputarlos (reemplazarlos por un valor generado por nosotros seg\u00fan cierta l\u00f3gica, por ejemplo la media de esa columna, etc). Lo m\u00e1s sencillo es quitar toda la fila, aunque esto depende de si nos lo podemos permitir en base\na la cantidad de datos que tenemos. En nuestro caso, como tenemos un n\u00famero considerable de filas, vamos a quitar todas las filas donde hay un NA en cualquiera de las columnas."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "columnas_limpiar = [\"dep_time\", \"dep_delay\", \"arr_time\", \"arr_delay\", \"air_time\", \"hour\", \"minute\"]\n\nflightsLimpiado = flightsDF\nfor nombreColumna in columnas_limpiar:  # para cada columna, nos quedamos con las filas que no tienen NA en esa columna\n    flightsLimpiado = flightsLimpiado.where(F.col(nombreColumna) != \"NA\")\n\nflightsLimpiado.cache()"}, {"cell_type": "markdown", "metadata": {}, "source": "Si ahora mostramos el n\u00famero de filas que tiene el DataFrame `flightsLimpiado` tras eliminar todas esas filas, vemos que ha disminuido ligeramente\npero sigue siendo un n\u00famero considerable como para realizar anal\u00edtica y sacar conclusiones sobre estos datos"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "flightsLimpiado.count()"}, {"cell_type": "markdown", "metadata": {}, "source": "Una vez que hemos eliminado los NA, vamos a convertir a tipo entero cada una de esas columnas que eran de tipo string. \nAhora no debe haber problema ya que todas las cadenas de texto contienen dentro un n\u00famero que puede ser convertido de texto a n\u00famero. Vamos tambi\u00e9n a convertir la columna `arr_delay` de tipo entero a n\u00famero real, necesario para los pasos posteriores donde ajustaremos un modelo predictivo."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "from pyspark.sql.types import IntegerType, DoubleType\n\nflightsConvertido = flightsLimpiado\n\nfor c in columnas_limpiar:\n    # m\u00e9todo que crea una columna o reemplaza una existente\n    flightsConvertido = flightsConvertido.withColumn(c, F.col(c).cast(IntegerType())) \n\nflightsConvertido = flightsConvertido.withColumn(\"arr_delay\", F.col(\"arr_delay\").cast(DoubleType()))\nflightsConvertido.cache()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "flightsConvertido.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "Vamos a volver a mostrar las 5 primeras filas del DataFrame limpio. Aparentemente son iguales a las que ya ten\u00edamos, pero ahora\nSpark s\u00ed est\u00e1 tratando como enteros las columnas que deber\u00edan serlo, y si queremos podemos hacer operaciones aritm\u00e9ticas\ncon ellas."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "flightsConvertido.show(5)"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "5810b869bd7baccabe0a2952dd0baae1", "grade": false, "grade_id": "cell-c0cfdd1db1edaa7d", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "### Ejercicio 1\n\nPartiendo del DataFrame `flightsConvertido` que ya tiene los tipos correctos en las columnas, se pide: \n\n* Crear un nuevo DataFrame llamado `aeropuertosOrigenDF` que tenga una columna `origin` y que tenga tantas filas como aeropuertos distintos de *origen* existan. \u00bfCu\u00e1ntas filas tiene? Almacenar dicho recuento en la variable entera `n_origen`.\n* Crear un nuevo DataFrame llamado `rutasDistintasDF` que tenga dos columnas `origin`, `dest` y que tenga tantas filas como rutas diferentes existan (es decir, como combinaciones distintas haya entre un origen y un destino). Una vez creado, contar cu\u00e1ntas combinaciones hay, almacenando dicho recuento en la variable entera `n_rutas`.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "8fb2ec5d49ff84edae4833eca797068b", "grade": false, "grade_id": "ejercicio-1", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "# Reemplaza None por el c\u00f3digo necesario para calcular sus valores correctos\naeropuertosOrigenDF = flightsConvertido.select('origin').distinct()\nn_origen = aeropuertosOrigenDF.count()\nrutasDistintasDF = flightsConvertido.select('dest',  'origin').distinct()\nn_rutas = rutasDistintasDF.count()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "print(n_origen, n_rutas)"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "f02d6e47bb3cc84e97f31cce091f80b3", "grade": true, "grade_id": "ejercicio-1-test", "locked": true, "points": 2, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "assert(n_origen == 2)\nassert(n_rutas == 115)\nassert(aeropuertosOrigenDF.count() == n_origen)\nassert(rutasDistintasDF.count() == n_rutas)"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "9229f0b48ed644e35a731b404738edf2", "grade": false, "grade_id": "cell-2b5f0dea18728fcf", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "### Ejercicio 2\n\n* Partiendo de nuevo de `flightsConvertido`, se pide calcular, *s\u00f3lo para los vuelos que llegan con* ***retraso positivo***, el retraso medio a la llegada de dichos vuelos, para cada aeropuerto de destino. La nueva columna con el retraso medio a la llegada debe llamarse `retraso_medio`. El DF resultante debe estar **ordenado de mayor a menor retraso medio**. El c\u00f3digo que calcule esto deber\u00eda ir encapsulado en una funci\u00f3n de Python llamada `retrasoMedio` que reciba como argumento un DataFrame y devuelva como resultado el DataFrame con el c\u00e1lculo descrito anteriormente.\n\n* Una vez hecha la funci\u00f3n, invocarla pas\u00e1ndole como argumento `flightsConvertido` y almacenar el resultado devuelto en la variable `retrasoMedioDF`."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "from pyspark.sql.functions import avg\n\ndf = flightsConvertido.select('arr_delay', 'dest')\\\n                      .where(F.col(\"arr_delay\") > 0)\\\n                      .groupby(F.col(\"dest\"))\\\n                      .agg(avg(\"arr_delay\").alias(\"retraso_medio\")) \n                      \nprint(type(df))\n    \ndf = df.sort(\"retraso_medio\", ascending=False) \ndf.show(5)\n\n# lista = df.take(3)\n\n# print(lista[0])"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "097436083d0007a7d99d5108e1a504c9", "grade": false, "grade_id": "ejercicio-2", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "def retrasoMedio(df):  \n    \n    df = flightsConvertido.select('arr_delay', 'dest')\\\n                      .where(F.col(\"arr_delay\") > 0)\\\n                      .groupby(F.col(\"dest\"))\\\n                      .agg(avg(\"arr_delay\").alias(\"retraso_medio\")) \n    df = df.sort(\"retraso_medio\", ascending=False)                       \n    return df \n\nlista = retrasoMedio(flightsConvertido).take(3)\nprint(lista)\n\nlista = retrasoMedio(flightsConvertido).take(3)\nprint((lista[0].retraso_medio), (lista[0].dest))\nprint((lista[1].retraso_medio), (lista[1].dest))\nprint((round(lista[2].retraso_medio, 2)), (lista[2].dest))"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "12aa19d467a0c50adface20495b6cf35", "grade": true, "grade_id": "ejercicio-2-tests", "locked": true, "points": 2, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "lista = retrasoMedio(flightsConvertido).take(3)\nassert((lista[0].retraso_medio == 64.75) & (lista[0].dest == \"BOI\"))\nassert((lista[1].retraso_medio == 46.8) & (lista[1].dest == \"HDN\"))\nassert((round(lista[2].retraso_medio, 2) == 41.19) & (lista[2].dest == \"SFO\"))"}, {"cell_type": "markdown", "metadata": {}, "source": "Ahora invocamos a nuestra funci\u00f3n `retrasoMedio` pas\u00e1ndole como argumento `flightsConvertido`. \u00bfCu\u00e1les son los tres aeropuertos con mayor retraso medio? \u00bfCu\u00e1les son sus retrasos medios en minutos?"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# 1. \u00bfCu\u00e1les son los tres aeropuertos con mayor retraso medio?\n\ndfMaxAvgDelayList = retrasoMedio(flightsConvertido).take(3)\n\nstr_print = ''\nlist_dest = []\nfor i in range(len(dfMaxAvgDelayList)):\n    str_print += dfMaxAvgDelayList[i].dest + ', '\n    list_dest.append(dfMaxAvgDelayList[i].dest)\n \nprint('1. Aeropuertos con mayor retraso medio: ' + str_print[0:len(str_print)-2])\n\n# 2. \u00bfCu\u00e1les son sus retrasos medios en minutos?\n\ndfMaxAvgDelayMinutes = flightsConvertido.select('minute', 'dest')\\\n                                        .where(F.col(\"dest\").isin(list_dest))\\\n                                        .groupby(F.col(\"dest\"))\\\n                                        .agg(avg(\"minute\").alias(\"minutes_mean\"))\\\n                                        .sort(\"dest\", ascending=False)\n\ndfMaxAvgDelayMinutesList = dfMaxAvgDelayMinutes.take(3)\n\nstr_print = ''\nlist_dest = []\nfor i in range(len(dfMaxAvgDelayMinutesList)):\n    str_print += str(round(dfMaxAvgDelayMinutesList[i].minutes_mean, 2)) + ', ' \n    \n\nprint('2. Retrasos medios en minutos: ' + str_print[0:len(str_print)-2])\n\ndfMaxAvgDelayMinutes.show(5)"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "fa9ed55cd86eb0958e150d6a918db1af", "grade": false, "grade_id": "cell-e577747d4427e32b", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "### Ejercicio 3\n\nAjustar un modelo de DecisionTree de Spark para predecir si un vuelo vendr\u00e1 o no con retraso (problema de clasificaci\u00f3n binaria), utilizando como variables predictoras el mes, el d\u00eda del mes, la hora de partida `dep_time`, la hora de llegada `arr_time`, el tipo de avi\u00f3n (`carrier`), la distancia y el tiempo que permanece en el aire. Para ello, sigue los siguientes pasos."}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "d4405b7db7180445df5cacc66d82db53", "grade": false, "grade_id": "cell-e577747d4427e32a", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "Notemos que en estos datos hay variables num\u00e9ricas y variables categ\u00f3ricas que ahora mismo est\u00e1n tipadas como num\u00e9ricas, como por ejemplo el mes del a\u00f1o (`month`), que es en realidad categ\u00f3rica. Debemos indicar a Spark cu\u00e1les son categ\u00f3ricas e indexarlas. Para ello se pide: \n\n* Crear un `StringIndexer` llamado `indexerMonth` y otro llamado `indexerCarrier` sobre las variables categ\u00f3ricas `month` y `carrier` (tipo de avi\u00f3n). El nombre de las columnas indexadas que se crear\u00e1n debe ser, respectivamente, `monthIndexed` y `carrierIndexed`."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "eb039584cd14e6bc3434e5be930341e6", "grade": false, "grade_id": "string-indexer", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "from pyspark.ml.feature import StringIndexer\n\nindexerMonth   = StringIndexer().setInputCol('month').setOutputCol('monthIndexed')\nindexerCarrier = StringIndexer().setInputCol('carrier').setOutputCol('carrierIndexed')\n\nprint(indexerMonth, indexerCarrier)"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "c96c7c7d5836922dd549b358b897b781", "grade": true, "grade_id": "string-indexer-tests", "locked": true, "points": 1, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "assert(isinstance(indexerMonth, StringIndexer))\nassert(isinstance(indexerCarrier, StringIndexer))\nassert(indexerMonth.getInputCol() == \"month\")\nassert(indexerMonth.getOutputCol() == \"monthIndexed\")\nassert(indexerCarrier.getInputCol() == \"carrier\")\nassert(indexerCarrier.getOutputCol() == \"carrierIndexed\")"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "69e024b0baeeb36bb74d136c7e113372", "grade": false, "grade_id": "cell-e577747d4427e323", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "Recordemos tambi\u00e9n que Spark requiere que todas las variables est\u00e9n en una \u00fanica columna de tipo vector, por lo que despu\u00e9s de indexar estas dos variables, tendremos que fusionar en una columna de tipo vector todas ellas, utilizando un `VectorAssembler`. Se pide:\n\n* Crear en una variable llamada `vectorAssembler` un `VectorAssembler` que reciba como entrada una lista de todas las variables de entrada (y que no debe incluir `arr_delay`) que ser\u00e1n las que formar\u00e1n parte del modelo. Crear primero esta lista de variables (lista de strings) en la variable `columnas_ensamblar` y pasar dicha variable como argumento al crear el `VectorAssembler`. Como es l\u00f3gico, en el caso de las columnas `month` y `carrier`, no usaremos las variables originales sino las indexadas en el apartado anterior. La columna de tipo vector creada con las caracter\u00edsticas ensambladas debe llamarse `features`."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "44623471d64d0605fcbcd4bbcc3c2a0d", "grade": false, "grade_id": "vector-assembler", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "from pyspark.ml.feature import VectorAssembler\n\ncolumnas_ensamblar = ['monthIndexed', 'day', 'dep_time', 'arr_time', 'carrierIndexed', 'distance', 'air_time']\nvectorAssembler = VectorAssembler().setInputCols(columnas_ensamblar)\\\n                                   .setOutputCol('features')\n\ninput_cols = vectorAssembler.getInputCols()\nprint(input_cols)"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "0d17b2fa1d8a4bd02b89952429ba1552", "grade": true, "grade_id": "vector-assembler-tests", "locked": true, "points": 1, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "assert(isinstance(vectorAssembler, VectorAssembler))\nassert(vectorAssembler.getOutputCol() == \"features\")\ninput_cols = vectorAssembler.getInputCols()\nassert(len(input_cols) == 7)\nassert(\"arr_delay\" not in input_cols)"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "8eeb3a3f0b15e8b374789706cd9bce49", "grade": false, "grade_id": "cell-e577747d4427e32dsdf", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "Finalmente, vemos que la columna `arr_delay` es continua, y no binaria como requiere un problema de clasificaci\u00f3n con dos clases. Vamos a convertirla en binaria. Para ello se pide:\n\n* Utilizar un binarizador de Spark, fijando a 15 el umbral, y guardarlo en la variable `delayBinarizer`. Consideramos retrasado un vuelo que ha llegado con m\u00e1s de 15 minutos de retraso, y no retrasado en caso contrario. La nueva columna creada con la variable binaria debe llamarse `arr_delay_binary` y debe ser interpretada como la columna target para nuestro algoritmo. Por ese motivo, esta columna **no** se incluy\u00f3 en el apartado anterior entre las columnas que se ensamblan para formar las features."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "1f33c493a3c3dd83fb6a39a7acefca5c", "grade": false, "grade_id": "binarizer", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "from pyspark.ml.feature import Binarizer\n\ndelayBinarizer = Binarizer().setInputCol('arr_delay')\\\n                            .setOutputCol('arr_delay_binary')\\\n                            .setThreshold(15.0)\n\nprint(delayBinarizer)"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "7ec408ab263c0378526f96bb5d374704", "grade": true, "grade_id": "binarizer-tests", "locked": true, "points": 1, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "assert(isinstance(delayBinarizer, Binarizer))\nassert(delayBinarizer.getThreshold() == 15)\nassert(delayBinarizer.getInputCol() == \"arr_delay\")\nassert(delayBinarizer.getOutputCol() == \"arr_delay_binary\")"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "5aed16d7ed0219fe1d8741848f594319", "grade": false, "grade_id": "cell-25a7793978ee7d05", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "Por \u00faltimo, crearemos el modelo de clasificaci\u00f3n.\n\n* Crear en una variable `decisionTree` un \u00e1rbol de clasificaci\u00f3n de Spark (`DecisionTreeClassifier` del paquete `pyspark.ml.classification`)\n* Indicar como columna de entrada la nueva columna creada por el `VectorAssembler` creado en un apartado anterior.\n* Indicar como columna objetivo (target) la nueva columna creada por el `Binarizer` del apartado anterior."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "e785136d6d06691c003ff9542027e03d", "grade": false, "grade_id": "decision-tree", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "from pyspark.ml.classification import DecisionTreeClassifier\n\ndecisionTree = DecisionTreeClassifier().setLabelCol(\"arr_delay_binary\").setFeaturesCol(\"features\")"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "2b03b48f304b5b34cd06eeec49e001fd", "grade": true, "grade_id": "decision-tree-tests", "locked": true, "points": 1, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "assert(isinstance(decisionTree, DecisionTreeClassifier))\nassert(decisionTree.getFeaturesCol() == \"features\")\nassert(decisionTree.getLabelCol() == \"arr_delay_binary\")"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "728da91edabbac5b62baf31bdd0a707e", "grade": false, "grade_id": "cell-e577747d4427e32d", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "Ahora vamos a encapsular todas las fases en un s\u00f3lo pipeline y procederemos a entrenarlo. Se pide:\n\n* Crear en una variable llamada `pipeline` un objeto `Pipeline` de Spark con las etapas anteriores en el orden adecuado para poder entrenar un modelo. \n\n* Entrenarlo invocando sobre ella al m\u00e9todo `fit` y guardar el pipeline entrenado devuelto por dicho m\u00e9todo en una variable llamada `pipelineModel`. \n\n* Aplicar el pipeline entrenado para transformar (predecir) el DataFrame `flightsConvertido`, guardando las predicciones devueltas en la variable `flightsPredictions` que ser\u00e1 un DataFrame. N\u00f3tese que estamos prediciendo los propios datos de entrenamiento y que, por simplicidad, no hab\u00edamos hecho (aunque habr\u00eda sido lo correcto) ninguna divisi\u00f3n de nuestros datos originales en subconjuntos distintos de entrenamiento y test antes de entrenar."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "edbdb627305d03efa41a88426330e160", "grade": false, "grade_id": "pipeline", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "from pyspark.ml import Pipeline\n\npipeline = Pipeline().setStages([indexerMonth, indexerCarrier, vectorAssembler, delayBinarizer, decisionTree])\npipelineModel = pipeline.fit(flightsConvertido)\nflightsPredictions = pipelineModel.transform(flightsConvertido)"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "107b5ec260550eb4235ffecff655289a", "grade": true, "grade_id": "pipeline-tests", "locked": true, "points": 2, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "from pyspark.ml import PipelineModel\nassert(isinstance(pipeline, Pipeline))\nassert(len(pipeline.getStages()) == 5)\nassert(isinstance(pipelineModel, PipelineModel))\nassert(\"probability\" in flightsPredictions.columns)\nassert(\"prediction\" in flightsPredictions.columns)\nassert(\"rawPrediction\" in flightsPredictions.columns)"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "0c531953abf18cfb3b67571ddde7a57d", "grade": false, "grade_id": "cell-61156fe5938763f1", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "Vamos a mostrar la matriz de confusi\u00f3n (este apartado no es evaluable). Agrupamos por la variable que tiene la clase verdadera y la que tiene la clase predicha, para ver en cu\u00e1ntos casos coinciden y en cu\u00e1ntos difieren."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "75f98ae39b827e75c3f0b4b2aaa6b0db", "grade": false, "grade_id": "cell-896752beb71cb455", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "flightsPredictions.groupBy(\"arr_delay_binary\", \"prediction\").count().show()"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.13"}}, "nbformat": 4, "nbformat_minor": 2}